{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# pip install tensorflow pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load your data\n",
        "# Assuming you have the data in a CSV or dataframe\n",
        "# If reading from CSV: df = pd.read_csv('your_data.csv')\n",
        "# Here let's manually simulate it based on your table\n",
        "\n",
        "df = pd.read_csv('/content/stock_news_sentiment_dataset (3).csv')  # <-- Change if needed\n"
      ],
      "metadata": {
        "id": "p8RdHmM4Haor"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing\n",
        "# Combine headline + description\n",
        "df['full_text'] = df['Headline'].astype(str) + \" \" + df['Description'].astype(str)\n",
        "\n",
        "# Encode sentiment\n",
        "df['Sentiment'] = df['Sentiment'].map({'Good': 1, 'Bad': 0})\n",
        "\n",
        "texts = df['full_text'].values\n",
        "labels = df['Sentiment'].values\n",
        "stock_prices = df['Stock Price'].values\n",
        "\n",
        "# 3. Tokenization\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 4. Padding\n",
        "max_length = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# 5. Define Risk Level\n",
        "def assign_risk(sentiment, stock_price):\n",
        "    if sentiment == 0 and stock_price < 100:\n",
        "        return 'High Risk'\n",
        "    elif sentiment == 1 and stock_price < 100:\n",
        "        return 'Medium Risk'\n",
        "    elif sentiment == 0 and stock_price >= 100:\n",
        "        return 'Medium Risk'\n",
        "    elif sentiment == 1 and stock_price >= 100:\n",
        "        return 'Low Risk'\n",
        "\n",
        "df['Risk_Level'] = df.apply(lambda row: assign_risk(row['Sentiment'], row['Stock Price']), axis=1)\n",
        "\n",
        "# 6. Split Train / Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# 7. Build LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_length))\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 8. Train Model\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    batch_size=4,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 9. Predict on test set\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(f\"âœ… Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "\n",
        "# 10. FINAL Function for Testing New Input\n",
        "\n",
        "def predict_sentiment_and_risk(headline, description, stock_price):\n",
        "    text = headline + \" \" + description\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    sentiment_pred = (model.predict(padded) > 0.5).astype(int)[0][0]\n",
        "\n",
        "    # Map prediction\n",
        "    sentiment_label = \"Good\" if sentiment_pred == 1 else \"Bad\"\n",
        "\n",
        "    # Assign Risk\n",
        "    risk = assign_risk(sentiment_pred, stock_price)\n",
        "\n",
        "    return sentiment_label, risk\n",
        "\n",
        "# Example Testing\n",
        "headline = \"3M Reports Record Revenue Growth in Healthcare Division\"\n",
        "description = \"The company announced its healthcare segment outperformed analyst expectations with a 10% YoY revenue growth.\"\n",
        "stock_price = 128.45\n",
        "\n",
        "sentiment, risk = predict_sentiment_and_risk(headline, description, stock_price)\n",
        "\n",
        "print(f\"\\nğŸ“¢ Predicted Sentiment: {sentiment}\")\n",
        "print(f\"âš¡ Assigned Risk Level: {risk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbeAg1A2IA89",
        "outputId": "33011997-d47b-470a-88a4-2b08740921d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 40ms/step - accuracy: 0.8379 - loss: 0.4729 - val_accuracy: 0.8729 - val_loss: 0.3780\n",
            "Epoch 2/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8592 - loss: 0.4104 - val_accuracy: 0.8729 - val_loss: 0.3819\n",
            "Epoch 3/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 41ms/step - accuracy: 0.8474 - loss: 0.4128 - val_accuracy: 0.8729 - val_loss: 0.3846\n",
            "Epoch 4/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.8456 - loss: 0.4085 - val_accuracy: 0.8729 - val_loss: 0.3910\n",
            "Epoch 5/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.8478 - loss: 0.4129 - val_accuracy: 0.8729 - val_loss: 0.4145\n",
            "Epoch 6/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.8507 - loss: 0.4376 - val_accuracy: 0.8729 - val_loss: 0.3817\n",
            "Epoch 7/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 42ms/step - accuracy: 0.8480 - loss: 0.4348 - val_accuracy: 0.8729 - val_loss: 0.3920\n",
            "Epoch 8/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.8275 - loss: 0.4635 - val_accuracy: 0.8729 - val_loss: 0.3816\n",
            "Epoch 9/10\n",
            "\u001b[1m102/480\u001b[0m \u001b[32mâ”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - accuracy: 0.8921 - loss: 0.3508"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Predict on Test Set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "# 2. Build test DataFrame (based on original df)\n",
        "# First, recreate original padded sequences' indexes\n",
        "test_indexes = y_test.index if hasattr(y_test, 'index') else np.arange(len(y_test))\n",
        "\n",
        "# Select corresponding rows from original DataFrame\n",
        "df_test = df.iloc[test_indexes].copy()  # make a copy to not mess up original\n",
        "\n",
        "# 3. Add Predictions\n",
        "df_test['True_Sentiment_Label'] = df_test['Sentiment'].map({1: 'Good', 0: 'Bad'})\n",
        "df_test['Predicted_Sentiment'] = y_pred\n",
        "df_test['Predicted_Sentiment_Label'] = df_test['Predicted_Sentiment'].map({1: 'Good', 0: 'Bad'})\n",
        "\n",
        "# 4. Assign Risk based on Predicted Sentiment\n",
        "df_test['Predicted_Risk_Level'] = df_test.apply(lambda row: assign_risk(row['Predicted_Sentiment'], row['Stock Price']), axis=1)\n",
        "\n",
        "# 5. Save full table\n",
        "df_test.to_csv('full_test_results_with_predictions.csv', index=False)\n",
        "\n",
        "print(\"âœ… Full test results saved to 'full_test_results_with_predictions.csv' successfully!\")\n"
      ],
      "metadata": {
        "id": "xy8KCFxtMfOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance beautifulsoup4 requests lxml newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOLhPDwZkC7g",
        "outputId": "4fa59441-63f2-4f0d-f956-39e2f33eea6b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZravAtkiO0",
        "outputId": "cf7286d3-79a0-4784-c033-3242bf3e0726"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k --no-cache-dir --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pclxGenmkjGj",
        "outputId": "32d8811a-464f-4bcd-b62e-5164d68fa926"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
            "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
            "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting PyYAML>=3.11 (from newspaper3k)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting lxml>=3.6.0 (from newspaper3k)\n",
            "  Downloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting nltk>=3.2.1 (from newspaper3k)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting requests>=2.10.0 (from newspaper3k)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dateutil>=2.5.3 (from newspaper3k)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting six (from feedfinder2>=0.0.4->newspaper3k)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m162.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m263.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m256.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m193.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m264.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m304.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m248.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m233.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m162.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m224.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m213.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m232.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m265.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m227.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m268.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m223.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m251.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=fcf12978e58434863b87a16b063bd45174fb44b6ddfa9711edb904b002bad7b4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_kgqp_44/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=c2423dbf4b9a4fad376502a47c359da83e8c74c94a8a23571767a20286d3ae9a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_kgqp_44/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=a0e09817ad911cee23e6e3e4ccdca9ec04a856ffcdfd4f580c12a3308af3cadf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_kgqp_44/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=bc60885cdcc530739ada4f5427813b086ea3191d4af28557205f34a07b04e5bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_kgqp_44/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, urllib3, typing-extensions, tqdm, soupsieve, six, regex, PyYAML, Pillow, lxml, joblib, idna, filelock, feedparser, cssselect, click, charset-normalizer, certifi, requests, python-dateutil, nltk, beautifulsoup4, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "  Attempting uninstall: tinysegmenter\n",
            "    Found existing installation: tinysegmenter 0.3\n",
            "    Uninstalling tinysegmenter-0.3:\n",
            "      Successfully uninstalled tinysegmenter-0.3\n",
            "  Attempting uninstall: sgmllib3k\n",
            "    Found existing installation: sgmllib3k 1.0.0\n",
            "    Uninstalling sgmllib3k-1.0.0:\n",
            "      Successfully uninstalled sgmllib3k-1.0.0\n",
            "  Attempting uninstall: jieba3k\n",
            "    Found existing installation: jieba3k 0.35.1\n",
            "    Uninstalling jieba3k-0.35.1:\n",
            "      Successfully uninstalled jieba3k-0.35.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.0\n",
            "    Uninstalling typing_extensions-4.13.0:\n",
            "      Successfully uninstalled typing_extensions-4.13.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.6\n",
            "    Uninstalling soupsieve-2.6:\n",
            "      Successfully uninstalled soupsieve-2.6\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.1\n",
            "    Uninstalling lxml-5.3.1:\n",
            "      Successfully uninstalled lxml-5.3.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: feedparser\n",
            "    Found existing installation: feedparser 6.0.11\n",
            "    Uninstalling feedparser-6.0.11:\n",
            "      Successfully uninstalled feedparser-6.0.11\n",
            "  Attempting uninstall: cssselect\n",
            "    Found existing installation: cssselect 1.3.0\n",
            "    Uninstalling cssselect-1.3.0:\n",
            "      Successfully uninstalled cssselect-1.3.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.3\n",
            "    Uninstalling beautifulsoup4-4.13.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.3\n",
            "  Attempting uninstall: requests-file\n",
            "    Found existing installation: requests-file 2.1.0\n",
            "    Uninstalling requests-file-2.1.0:\n",
            "      Successfully uninstalled requests-file-2.1.0\n",
            "  Attempting uninstall: feedfinder2\n",
            "    Found existing installation: feedfinder2 0.0.4\n",
            "    Uninstalling feedfinder2-0.0.4:\n",
            "      Successfully uninstalled feedfinder2-0.0.4\n",
            "  Attempting uninstall: tldextract\n",
            "    Found existing installation: tldextract 5.1.3\n",
            "    Uninstalling tldextract-5.1.3:\n",
            "      Successfully uninstalled tldextract-5.1.3\n",
            "  Attempting uninstall: newspaper3k\n",
            "    Found existing installation: newspaper3k 0.2.8\n",
            "    Uninstalling newspaper3k-0.2.8:\n",
            "      Successfully uninstalled newspaper3k-0.2.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-11.1.0 PyYAML-6.0.2 beautifulsoup4-4.13.3 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.18.0 idna-3.10 jieba3k-0.35.1 joblib-1.4.2 lxml-5.3.2 newspaper3k-0.2.8 nltk-3.9.1 python-dateutil-2.9.0.post0 regex-2024.11.6 requests-2.32.3 requests-file-2.1.0 sgmllib3k-1.0.0 six-1.17.0 soupsieve-2.6 tinysegmenter-0.3 tldextract-5.1.3 tqdm-4.67.1 typing-extensions-4.13.1 urllib3-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "feedparser",
                  "joblib",
                  "lxml",
                  "nltk",
                  "regex",
                  "requests",
                  "requests_file",
                  "sgmllib",
                  "six",
                  "tldextract",
                  "tqdm"
                ]
              },
              "id": "02b298e41bb0460d9a4dffd65d03c39a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article  # <-- New library for extracting full article text\n",
        "\n",
        "# Function to fetch stock price\n",
        "def fetch_stock_price(ticker):\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        todays_data = stock.history(period='1d')\n",
        "        return todays_data['Close'].iloc[0]\n",
        "    except Exception as e:\n",
        "        print(f\"â— Error fetching stock price: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to fetch latest headline + link\n",
        "def fetch_latest_headline_and_link(ticker):\n",
        "    try:\n",
        "        url = f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        news_table = soup.find(id=\"news-table\")\n",
        "        first_news = news_table.find('tr')\n",
        "\n",
        "        if first_news and first_news.a:\n",
        "            headline = first_news.a.text\n",
        "            link = first_news.a['href']\n",
        "            return headline, link\n",
        "        else:\n",
        "            return \"No news found\", None\n",
        "    except Exception as e:\n",
        "        print(f\"â— Error fetching news: {e}\")\n",
        "        return \"No news found\", None\n",
        "\n",
        "# Function to extract article description from link\n",
        "def fetch_article_description(link):\n",
        "    if not link:\n",
        "        return \"No description available.\"\n",
        "    try:\n",
        "        article = Article(link)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        print(f\"â— Error fetching article description: {e}\")\n",
        "        return \"No description available.\"\n",
        "\n",
        "# Main function\n",
        "def predict_from_stock_input_full():\n",
        "    print(\"ğŸ“‹ Please enter your stock symbol:\")\n",
        "\n",
        "    # Take stock symbol input\n",
        "    company = input(\"Enter Stock Symbol (e.g., MMM): \").strip().upper()\n",
        "\n",
        "    # Fetch stock price\n",
        "    stock_price = fetch_stock_price(company)\n",
        "    if stock_price is None:\n",
        "        print(\"â— Could not fetch stock price. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch latest headline + link\n",
        "    headline, link = fetch_latest_headline_and_link(company)\n",
        "\n",
        "    if link:\n",
        "        description = fetch_article_description(link)\n",
        "    else:\n",
        "        description = headline  # fallback if no link\n",
        "\n",
        "    print(f\"\\nğŸ” Fetched Data for {company}:\")\n",
        "    print(f\"Stock Price: {stock_price}\")\n",
        "    print(f\"Latest News Headline: {headline}\")\n",
        "    print(f\"News Description: {description[:200]}...\")  # Print first 200 chars\n",
        "\n",
        "    # Combine headline + description\n",
        "    full_text = headline + \" \" + description\n",
        "\n",
        "    # Tokenize and pad\n",
        "    seq = tokenizer.texts_to_sequences([full_text])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Predict sentiment\n",
        "    sentiment_pred = (model.predict(padded) > 0.5).astype(int)[0][0]\n",
        "    sentiment_label = \"Good\" if sentiment_pred == 1 else \"Bad\"\n",
        "\n",
        "    # Predict risk\n",
        "    risk_level = assign_risk(sentiment_pred, stock_price)\n",
        "\n",
        "    print(\"\\nâœ… Prediction Complete:\")\n",
        "    print(f\"Company: {company}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment_label}\")\n",
        "    print(f\"Assigned Risk Level: {risk_level}\")\n",
        "\n",
        "# Call the final full function\n",
        "predict_from_stock_input_full()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSBTVZA3j-Ws",
        "outputId": "5ba76161-bfcc-4fa4-e5c1-a769a4090113"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ Please enter your stock symbol:\n",
            "Enter Stock Symbol (e.g., MMM): AAPL\n",
            "â— Error fetching article description: Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/article/4773203-cii-looking-like-tempting-opportunity?utm_source=finviz.com&utm_medium=referral&feed_item_type=article on URL https://seekingalpha.com/article/4773203-cii-looking-like-tempting-opportunity?utm_source=finviz.com&utm_medium=referral&feed_item_type=article\n",
            "\n",
            "ğŸ” Fetched Data for AAPL:\n",
            "Stock Price: 188.3800048828125\n",
            "Latest News Headline: CII: Looking Like A Tempting Opportunity\n",
            "News Description: No description available....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\n",
            "âœ… Prediction Complete:\n",
            "Company: AAPL\n",
            "Predicted Sentiment: Good\n",
            "Assigned Risk Level: Low Risk\n"
          ]
        }
      ]
    }
  ]
}