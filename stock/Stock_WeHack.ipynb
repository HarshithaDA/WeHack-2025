{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# pip install tensorflow pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load your data\n",
        "df = pd.read_csv('/content/stock_news_sentiment_dataset (3).csv')  # <-- Change if needed\n"
      ],
      "metadata": {
        "id": "p8RdHmM4Haor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing\n",
        "# Combine headline + description\n",
        "df['full_text'] = df['Headline'].astype(str) + \" \" + df['Description'].astype(str)\n",
        "\n",
        "# Encode sentiment\n",
        "df['Sentiment'] = df['Sentiment'].map({'Good': 1, 'Bad': 0})\n",
        "\n",
        "texts = df['full_text'].values\n",
        "labels = df['Sentiment'].values\n",
        "stock_prices = df['Stock Price'].values\n",
        "\n",
        "# 3. Tokenization\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 4. Padding\n",
        "max_length = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# 5. Define Risk Level\n",
        "def assign_risk(sentiment, stock_price):\n",
        "    if sentiment == 0 and stock_price < 100:\n",
        "        return 'High Risk'\n",
        "    elif sentiment == 1 and stock_price < 100:\n",
        "        return 'Medium Risk'\n",
        "    elif sentiment == 0 and stock_price >= 100:\n",
        "        return 'Medium Risk'\n",
        "    elif sentiment == 1 and stock_price >= 100:\n",
        "        return 'Low Risk'\n",
        "\n",
        "df['Risk_Level'] = df.apply(lambda row: assign_risk(row['Sentiment'], row['Stock Price']), axis=1)\n",
        "\n",
        "# 6. Split Train / Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# 7. Build LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_length))\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 8. Train Model\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    batch_size=4,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 9. Predict on test set\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "print(f\"✅ Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "\n",
        "# 10. FINAL Function for Testing New Input\n",
        "\n",
        "def predict_sentiment_and_risk(headline, description, stock_price):\n",
        "    text = headline + \" \" + description\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    sentiment_pred = (model.predict(padded) > 0.5).astype(int)[0][0]\n",
        "\n",
        "    # Map prediction\n",
        "    sentiment_label = \"Good\" if sentiment_pred == 1 else \"Bad\"\n",
        "\n",
        "    # Assign Risk\n",
        "    risk = assign_risk(sentiment_pred, stock_price)\n",
        "\n",
        "    return sentiment_label, risk\n",
        "\n",
        "# Example Testing\n",
        "headline = \"3M Reports Record Revenue Growth in Healthcare Division\"\n",
        "description = \"The company announced its healthcare segment outperformed analyst expectations with a 10% YoY revenue growth.\"\n",
        "stock_price = 128.45\n",
        "\n",
        "sentiment, risk = predict_sentiment_and_risk(headline, description, stock_price)\n",
        "\n",
        "print(f\"\\n📢 Predicted Sentiment: {sentiment}\")\n",
        "print(f\"⚡ Assigned Risk Level: {risk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbeAg1A2IA89",
        "outputId": "b840251e-5b4b-40da-a5cd-170d09180f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 41ms/step - accuracy: 0.8569 - loss: 0.4359 - val_accuracy: 0.8729 - val_loss: 0.3782\n",
            "Epoch 2/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.8591 - loss: 0.4054 - val_accuracy: 0.8729 - val_loss: 0.4203\n",
            "Epoch 3/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.8428 - loss: 0.4393 - val_accuracy: 0.8729 - val_loss: 0.3684\n",
            "Epoch 4/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 41ms/step - accuracy: 0.8397 - loss: 0.3959 - val_accuracy: 0.8687 - val_loss: 0.3818\n",
            "Epoch 5/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.8480 - loss: 0.3693 - val_accuracy: 0.8667 - val_loss: 0.3693\n",
            "Epoch 6/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.8604 - loss: 0.3905 - val_accuracy: 0.8750 - val_loss: 0.3415\n",
            "Epoch 7/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.8885 - loss: 0.3274 - val_accuracy: 0.8938 - val_loss: 0.3102\n",
            "Epoch 8/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.9096 - loss: 0.3050 - val_accuracy: 0.8687 - val_loss: 0.3694\n",
            "Epoch 9/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.9096 - loss: 0.2973 - val_accuracy: 0.8667 - val_loss: 0.3902\n",
            "Epoch 10/10\n",
            "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 41ms/step - accuracy: 0.8985 - loss: 0.3219 - val_accuracy: 0.8854 - val_loss: 0.3574\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "✅ Test Accuracy: 86.50%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\n",
            "📢 Predicted Sentiment: Good\n",
            "⚡ Assigned Risk Level: Low Risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Predict on Test Set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "# 2. Build test DataFrame (based on original df)\n",
        "# First, recreate original padded sequences' indexes\n",
        "test_indexes = y_test.index if hasattr(y_test, 'index') else np.arange(len(y_test))\n",
        "\n",
        "# Select corresponding rows from original DataFrame\n",
        "df_test = df.iloc[test_indexes].copy()  # make a copy to not mess up original\n",
        "\n",
        "# 3. Add Predictions\n",
        "df_test['True_Sentiment_Label'] = df_test['Sentiment'].map({1: 'Good', 0: 'Bad'})\n",
        "df_test['Predicted_Sentiment'] = y_pred\n",
        "df_test['Predicted_Sentiment_Label'] = df_test['Predicted_Sentiment'].map({1: 'Good', 0: 'Bad'})\n",
        "\n",
        "# 4. Assign Risk based on Predicted Sentiment\n",
        "df_test['Predicted_Risk_Level'] = df_test.apply(lambda row: assign_risk(row['Predicted_Sentiment'], row['Stock Price']), axis=1)\n",
        "\n",
        "# 5. Save full table\n",
        "df_test.to_csv('full_test_results_with_predictions.csv', index=False)\n",
        "\n",
        "print(\"✅ Full test results saved to 'full_test_results_with_predictions.csv' successfully!\")\n"
      ],
      "metadata": {
        "id": "xy8KCFxtMfOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e576a2-b552-4c87-b0f7-798689117f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "✅ Full test results saved to 'full_test_results_with_predictions.csv' successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance beautifulsoup4 requests lxml newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOLhPDwZkC7g",
        "outputId": "1b419525-d695-417e-f259-86f872ae241a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZravAtkiO0",
        "outputId": "94a6cf01-c90a-4955-e1c4-638b96ff3ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.2)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k --no-cache-dir --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pclxGenmkjGj",
        "outputId": "3bfcc4c3-643c-40b0-c05d-7b3d2e9dbce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
            "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
            "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting PyYAML>=3.11 (from newspaper3k)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting lxml>=3.6.0 (from newspaper3k)\n",
            "  Downloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting nltk>=3.2.1 (from newspaper3k)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting requests>=2.10.0 (from newspaper3k)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dateutil>=2.5.3 (from newspaper3k)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting six (from feedfinder2>=0.0.4->newspaper3k)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m192.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=6cfe18a385347bde2752535240dc6fe0be0547d570d2b10abb15dc18f282b7b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i6w1_ht9/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=62621cf5ba46c61a1c8a1ba30889aebd0a2072fe55920da129a4646d7490513e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i6w1_ht9/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=e51a8ac85027d8dc6c174d9c2df95a5822ba997a37c9fab8a8126294a80b9a3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i6w1_ht9/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=6f823066ce09f1cd5d0ec3e8b69c1828f313b55d16b93394eacaf31ccc8da5e8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i6w1_ht9/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, urllib3, typing-extensions, tqdm, soupsieve, six, regex, PyYAML, Pillow, lxml, joblib, idna, filelock, feedparser, cssselect, click, charset-normalizer, certifi, requests, python-dateutil, nltk, beautifulsoup4, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "  Attempting uninstall: tinysegmenter\n",
            "    Found existing installation: tinysegmenter 0.3\n",
            "    Uninstalling tinysegmenter-0.3:\n",
            "      Successfully uninstalled tinysegmenter-0.3\n",
            "  Attempting uninstall: sgmllib3k\n",
            "    Found existing installation: sgmllib3k 1.0.0\n",
            "    Uninstalling sgmllib3k-1.0.0:\n",
            "      Successfully uninstalled sgmllib3k-1.0.0\n",
            "  Attempting uninstall: jieba3k\n",
            "    Found existing installation: jieba3k 0.35.1\n",
            "    Uninstalling jieba3k-0.35.1:\n",
            "      Successfully uninstalled jieba3k-0.35.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.1\n",
            "    Uninstalling typing_extensions-4.13.1:\n",
            "      Successfully uninstalled typing_extensions-4.13.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.6\n",
            "    Uninstalling soupsieve-2.6:\n",
            "      Successfully uninstalled soupsieve-2.6\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.2\n",
            "    Uninstalling lxml-5.3.2:\n",
            "      Successfully uninstalled lxml-5.3.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: feedparser\n",
            "    Found existing installation: feedparser 6.0.11\n",
            "    Uninstalling feedparser-6.0.11:\n",
            "      Successfully uninstalled feedparser-6.0.11\n",
            "  Attempting uninstall: cssselect\n",
            "    Found existing installation: cssselect 1.3.0\n",
            "    Uninstalling cssselect-1.3.0:\n",
            "      Successfully uninstalled cssselect-1.3.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.3\n",
            "    Uninstalling beautifulsoup4-4.13.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.3\n",
            "  Attempting uninstall: requests-file\n",
            "    Found existing installation: requests-file 2.1.0\n",
            "    Uninstalling requests-file-2.1.0:\n",
            "      Successfully uninstalled requests-file-2.1.0\n",
            "  Attempting uninstall: feedfinder2\n",
            "    Found existing installation: feedfinder2 0.0.4\n",
            "    Uninstalling feedfinder2-0.0.4:\n",
            "      Successfully uninstalled feedfinder2-0.0.4\n",
            "  Attempting uninstall: tldextract\n",
            "    Found existing installation: tldextract 5.1.3\n",
            "    Uninstalling tldextract-5.1.3:\n",
            "      Successfully uninstalled tldextract-5.1.3\n",
            "  Attempting uninstall: newspaper3k\n",
            "    Found existing installation: newspaper3k 0.2.8\n",
            "    Uninstalling newspaper3k-0.2.8:\n",
            "      Successfully uninstalled newspaper3k-0.2.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-11.1.0 PyYAML-6.0.2 beautifulsoup4-4.13.3 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.18.0 idna-3.10 jieba3k-0.35.1 joblib-1.4.2 lxml-5.3.2 newspaper3k-0.2.8 nltk-3.9.1 python-dateutil-2.9.0.post0 regex-2024.11.6 requests-2.32.3 requests-file-2.1.0 sgmllib3k-1.0.0 six-1.17.0 soupsieve-2.6 tinysegmenter-0.3 tldextract-5.1.3 tqdm-4.67.1 typing-extensions-4.13.1 urllib3-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "feedparser",
                  "joblib",
                  "lxml",
                  "requests",
                  "requests_file",
                  "sgmllib",
                  "six",
                  "tldextract"
                ]
              },
              "id": "a18642283f604fbb9840dbb3ea6410b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "# Function to fetch stock price\n",
        "def fetch_stock_price(ticker):\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        todays_data = stock.history(period='1d')\n",
        "        return todays_data['Close'].iloc[0]\n",
        "    except Exception as e:\n",
        "        print(f\"❗ Error fetching stock price: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to fetch latest headline + link\n",
        "def fetch_latest_headline_and_link(ticker):\n",
        "    try:\n",
        "        url = f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        news_table = soup.find(id=\"news-table\")\n",
        "        first_news = news_table.find('tr')\n",
        "\n",
        "        if first_news and first_news.a:\n",
        "            headline = first_news.a.text\n",
        "            link = first_news.a['href']\n",
        "            return headline, link\n",
        "        else:\n",
        "            return \"No news found\", None\n",
        "    except Exception as e:\n",
        "        print(f\"❗ Error fetching news: {e}\")\n",
        "        return \"No news found\", None\n",
        "\n",
        "# Function to extract article description from link\n",
        "def fetch_article_description(link):\n",
        "    if not link:\n",
        "        return \"No description available.\"\n",
        "    try:\n",
        "        article = Article(link)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        print(f\"❗ Error fetching article description: {e}\")\n",
        "        return \"No description available.\"\n",
        "\n",
        "# Main function\n",
        "def predict_from_stock_input_full():\n",
        "    print(\"📋 Please enter your stock symbol:\")\n",
        "\n",
        "    # Take stock symbol input\n",
        "    company = input(\"Enter Stock Symbol (e.g., MMM): \").strip().upper()\n",
        "\n",
        "    # Fetch stock price\n",
        "    stock_price = fetch_stock_price(company)\n",
        "    if stock_price is None:\n",
        "        print(\"❗ Could not fetch stock price. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch latest headline + link\n",
        "    headline, link = fetch_latest_headline_and_link(company)\n",
        "\n",
        "    if link:\n",
        "        description = fetch_article_description(link)\n",
        "    else:\n",
        "        description = headline  # fallback if no link\n",
        "\n",
        "    print(f\"\\n🔎 Fetched Data for {company}:\")\n",
        "    print(f\"Stock Price: {stock_price}\")\n",
        "    print(f\"Latest News Headline: {headline}\")\n",
        "    print(f\"News Description: {description[:200]}...\")  # Print first 200 chars\n",
        "\n",
        "    # Combine headline + description\n",
        "    full_text = headline + \" \" + description\n",
        "\n",
        "    # Tokenize and pad\n",
        "    seq = tokenizer.texts_to_sequences([full_text])\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Predict sentiment\n",
        "    sentiment_pred = (model.predict(padded) > 0.5).astype(int)[0][0]\n",
        "    sentiment_label = \"Good\" if sentiment_pred == 1 else \"Bad\"\n",
        "\n",
        "    # Predict risk\n",
        "    risk_level = assign_risk(sentiment_pred, stock_price)\n",
        "\n",
        "    print(\"\\n✅ Prediction Complete:\")\n",
        "    print(f\"Company: {company}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment_label}\")\n",
        "    print(f\"Assigned Risk Level: {risk_level}\")\n",
        "\n",
        "# Call the final full function\n",
        "predict_from_stock_input_full()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSBTVZA3j-Ws",
        "outputId": "f428d1dc-f8c3-4e0a-8459-4bcd4852ceb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Please enter your stock symbol:\n",
            "Enter Stock Symbol (e.g., MMM): AAPL\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook': No scheme supplied. Perhaps you meant https:///news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook? on URL /news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook\n",
            "\n",
            "🔎 Fetched Data for AAPL:\n",
            "Stock Price: 188.3800048828125\n",
            "Latest News Headline: Think You Know Apple? Here's 1 Lesser-Known Fact You Shouldn't Overlook.\n",
            "News Description: No description available....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\n",
            "✅ Prediction Complete:\n",
            "Company: AAPL\n",
            "Predicted Sentiment: Good\n",
            "Assigned Risk Level: Low Risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation"
      ],
      "metadata": {
        "id": "GsDpo_SLuAEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 1: Predefined List of Popular Stocks to scan\n",
        "popular_stocks = ['AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'META', 'GOOGL', 'NFLX', 'AMD', 'CRM']\n",
        "\n",
        "# STEP 2: Recommendation Function\n",
        "def recommend_top_stocks():\n",
        "    results = []\n",
        "\n",
        "    print(\"🔎 Scanning popular stocks for recommendations...\")\n",
        "\n",
        "    for ticker in popular_stocks:\n",
        "        try:\n",
        "            stock_price = fetch_stock_price(ticker)\n",
        "            if stock_price is None:\n",
        "                continue\n",
        "\n",
        "            headline, link = fetch_latest_headline_and_link(ticker)\n",
        "\n",
        "            if link:\n",
        "                description = fetch_article_description(link)\n",
        "            else:\n",
        "                description = headline  # fallback\n",
        "\n",
        "            full_text = headline + \" \" + description\n",
        "\n",
        "            seq = tokenizer.texts_to_sequences([full_text])\n",
        "            padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "            sentiment_pred = (model.predict(padded) > 0.5).astype(int)[0][0]\n",
        "            sentiment_score = model.predict(padded)[0][0]  # Get probability between 0 and 1\n",
        "            sentiment_label = \"Good\" if sentiment_pred == 1 else \"Bad\"\n",
        "\n",
        "            risk_level = assign_risk(sentiment_pred, stock_price)\n",
        "\n",
        "            results.append({\n",
        "                'Ticker': ticker,\n",
        "                'Stock Price': stock_price,\n",
        "                'Predicted Sentiment': sentiment_label,\n",
        "                'Sentiment Probability': sentiment_score,\n",
        "                'Risk Level': risk_level\n",
        "            })\n",
        "\n",
        "            print(f\"✅ {ticker}: Sentiment={sentiment_label}, Risk={risk_level}, Price=${stock_price:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❗ Error processing {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # STEP 3: Create DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    if df_results.empty:\n",
        "        print(\"❗ No stocks could be recommended.\")\n",
        "        return\n",
        "\n",
        "    # STEP 4: Filter Low/Medium Risk and Good Sentiment\n",
        "    df_filtered = df_results[\n",
        "        (df_results['Risk Level'].isin(['Low Risk', 'Medium Risk'])) &\n",
        "        (df_results['Predicted Sentiment'] == 'Good')\n",
        "    ]\n",
        "\n",
        "    # STEP 5: Sort by sentiment probability (confidence) and pick Top 3\n",
        "    top_picks = df_filtered.sort_values(by='Sentiment Probability', ascending=False).head(3)\n",
        "\n",
        "    if top_picks.empty:\n",
        "        print(\"❗ No Low/Medium Risk Good stocks found.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n🏆 Top 3 Recommended Stocks to Invest In:\")\n",
        "    print(top_picks[['Ticker', 'Stock Price', 'Predicted Sentiment', 'Risk Level']])\n",
        "\n",
        "# STEP 6: Call the recommendation function\n",
        "recommend_top_stocks()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9d0fuL9uBHJ",
        "outputId": "5a4b0a1d-7fef-4e0b-dd6d-9d6c24fa7eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Scanning popular stocks for recommendations...\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook': No scheme supplied. Perhaps you meant https:///news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook? on URL /news/18545/think-you-know-apple-heres-1-lesser-known-fact-you-shouldnt-overlook\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "✅ AAPL: Sentiment=Good, Risk=Low Risk, Price=$188.38\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "✅ MSFT: Sentiment=Good, Risk=Low Risk, Price=$359.84\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "✅ NVDA: Sentiment=Good, Risk=Medium Risk, Price=$94.31\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "✅ TSLA: Sentiment=Good, Risk=Low Risk, Price=$239.43\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run': No scheme supplied. Perhaps you meant https:///news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run? on URL /news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "✅ AMZN: Sentiment=Good, Risk=Low Risk, Price=$171.00\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run': No scheme supplied. Perhaps you meant https:///news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run? on URL /news/18547/3-top-bargain-tech-stocks-ready-for-the-next-bull-run\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "✅ META: Sentiment=Good, Risk=Low Risk, Price=$504.73\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18556/2-growth-stocks-that-could-go-parabolic': No scheme supplied. Perhaps you meant https:///news/18556/2-growth-stocks-that-could-go-parabolic? on URL /news/18556/2-growth-stocks-that-could-go-parabolic\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "✅ GOOGL: Sentiment=Good, Risk=Low Risk, Price=$145.60\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18520/market-turmoil-3-stocks-to-steady-any-portfolio': No scheme supplied. Perhaps you meant https:///news/18520/market-turmoil-3-stocks-to-steady-any-portfolio? on URL /news/18520/market-turmoil-3-stocks-to-steady-any-portfolio\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "✅ NFLX: Sentiment=Good, Risk=Low Risk, Price=$855.86\n",
            "❗ Error fetching article description: Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/article/4773288-the-impact-of-tariffs-on-chipmakers?utm_source=finviz.com&utm_medium=referral&feed_item_type=article on URL https://seekingalpha.com/article/4773288-the-impact-of-tariffs-on-chipmakers?utm_source=finviz.com&utm_medium=referral&feed_item_type=article\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "✅ AMD: Sentiment=Good, Risk=Medium Risk, Price=$85.76\n",
            "❗ Error fetching article description: Article `download()` failed with Invalid URL '/news/18471/is-salesforce-inc-crm-the-best-enterprise-software-stock-to-buy-now': No scheme supplied. Perhaps you meant https:///news/18471/is-salesforce-inc-crm-the-best-enterprise-software-stock-to-buy-now? on URL /news/18471/is-salesforce-inc-crm-the-best-enterprise-software-stock-to-buy-now\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "✅ CRM: Sentiment=Good, Risk=Low Risk, Price=$240.76\n",
            "\n",
            "🏆 Top 3 Recommended Stocks to Invest In:\n",
            "  Ticker  Stock Price Predicted Sentiment Risk Level\n",
            "3   TSLA   239.429993                Good   Low Risk\n",
            "4   AMZN   171.000000                Good   Low Risk\n",
            "5   META   504.730011                Good   Low Risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xkApH6ruBdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}